# ğŸ§  LLM-Powered Document Q&A System

A fully , private, and intelligent question-answering system for personal documents â€” powered by local LLMs via **Ollama** and advanced **RAG (Retrieval Augmented Generation)** using **LlamaIndex**.

> Upload your files. Ask any question. Get contextual answers. No cloud, no compromise.

---

## âœ¨ Features

- ğŸ“„ **Document Upload:** Upload `.txt` and `.pdf` files through a sleek web interface.
- ğŸ¤– **Local LLM Integration:** Run LLMs on your machine using **Ollama** â€” no API keys, no external servers.
- ğŸ” **RAG Pipeline:** Context-aware answers using **LlamaIndex** to retrieve relevant info before generating answers.
- ğŸ§¬ **Local Embeddings:** Uses **sentence-transformers/all-MiniLM-L6-v2** for document indexing and search.
- âš™ï¸ **Flask Backend:** Lightweight Python web server managing files and API endpoints.
- ğŸ–¥ï¸ **User-Friendly UI:** Minimal HTML/JavaScript interface for interaction.

---

## ğŸ§± Architecture Overview

![flowchart bg](https://github.com/user-attachments/assets/1ecd9760-cfa1-44da-8c25-8f054cb8801a)



### ğŸ§© Components Breakdown

- **Frontend:** `index.html` â€” Upload files, input questions, display answers.
- **Backend:** `app.py` â€” Flask handles routing, indexing, and LLM queries.
- **Storage:** `uploaded_docs/` â€” Temporary folder to store uploaded files.
- **LlamaIndex:** Reads, indexes, and retrieves context from uploaded docs.
- **Embedding Model:** Local HuggingFace embedding for efficient vector search.
- **LLM via Ollama:** Self-hosted inference using models like `llama2` or `tinyllama`.

---

## ğŸš€ Getting Started

### ğŸ”§ Prerequisites

- **Python** â‰¥ 3.8
- **pip** (comes with Python)
- **Ollama** installed and running locally:  
  ğŸ‘‰ [https://ollama.com](https://ollama.com)

### ğŸ§  Install a Local Model

```bash
ollama pull tinyllama
# OR for better answers (needs 16GB+ RAM):
# ollama pull llama2
```

---

### ğŸ“¦ Installation

1. **Clone the repository:**

```bash
git clone <your-repo-link>
cd <your-repo-name>
```

2. **Create and activate a virtual environment:**

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

3. **Install dependencies:**

```bash
pip install -r requirements.txt
```

---

## âš™ï¸ Running the Application

1. **Ensure Ollama is running** in the background.

2. **Open `app.py` and configure your model:**

```python
Settings.llm = Ollama(model="tinyllama", request_timeout=360.0)
# OR
# Settings.llm = Ollama(model="llama2", request_timeout=360.0)
```

3. **Run the Flask server:**

```bash
python app.py
```

4. **Open your browser** and navigate to:

```
http://127.0.0.1:5000/
```

---

## ğŸ§ª How to Use

1. **Upload a document** (`.txt` or `.pdf`).
2. **Ask questions** about its content.
3. **Receive answers** generated by your local LLM using retrieved context.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app.py                   # Flask backend logic
â”œâ”€â”€ index.html               # Frontend user interface
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ uploaded_docs/           # Auto-created folder for uploaded files
â””â”€â”€ README.md                # This documentation
```

---

## âš ï¸ Troubleshooting & Tips

- ğŸ§  **First query slow?** Initial LLM load takes time. Later queries will be faster.
- ğŸ§  **Memory errors?** Use `tinyllama` if your system has <16GB RAM.
- ğŸ” **Uploading a new doc?** The old index gets reset each time.
- ğŸš¨ **"No LLM response" errors?** Make sure Ollama is running before starting Flask.

---

## ğŸ‘ Contributing

Feel free to fork, raise issues, or submit PRs. This project is a great starting point for building more advanced AI-native tools.

---

## ğŸ–¤ Credits

- **LlamaIndex** - https://www.llamaindex.ai/
- **Ollama** - https://ollama.com/
- **HuggingFace** - https://huggingface.co/

---

## ğŸ“œ License

MIT License â€” use it, tweak it, build on it.

---

> Built with ğŸ’» + ğŸ–¤ for developers who want private, powerful AI tools without relying on the cloud.
