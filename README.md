# 🧠 LLM-Powered Document Q&A System

A fully , private, and intelligent question-answering system for personal documents — powered by local LLMs via **Ollama** and advanced **RAG (Retrieval Augmented Generation)** using **LlamaIndex**.

> Upload your files. Ask any question. Get contextual answers. No cloud, no compromise.

---

## ✨ Features

- 📄 **Document Upload:** Upload `.txt` and `.pdf` files through a sleek web interface.
- 🤖 **Local LLM Integration:** Run LLMs on your machine using **Ollama** — no API keys, no external servers.
- 🔍 **RAG Pipeline:** Context-aware answers using **LlamaIndex** to retrieve relevant info before generating answers.
- 🧬 **Local Embeddings:** Uses **sentence-transformers/all-MiniLM-L6-v2** for document indexing and search.
- ⚙️ **Flask Backend:** Lightweight Python web server managing files and API endpoints.
- 🖥️ **User-Friendly UI:** Minimal HTML/JavaScript interface for interaction.

---

## 🧱 Architecture Overview

![flowchart bg](https://github.com/user-attachments/assets/1ecd9760-cfa1-44da-8c25-8f054cb8801a)



### 🧩 Components Breakdown

- **Frontend:** `index.html` — Upload files, input questions, display answers.
- **Backend:** `app.py` — Flask handles routing, indexing, and LLM queries.
- **Storage:** `uploaded_docs/` — Temporary folder to store uploaded files.
- **LlamaIndex:** Reads, indexes, and retrieves context from uploaded docs.
- **Embedding Model:** Local HuggingFace embedding for efficient vector search.
- **LLM via Ollama:** Self-hosted inference using models like `llama2` or `tinyllama`.

---

## 🚀 Getting Started

### 🔧 Prerequisites

- **Python** ≥ 3.8
- **pip** (comes with Python)
- **Ollama** installed and running locally:  
  👉 [https://ollama.com](https://ollama.com)

### 🧠 Install a Local Model

```bash
ollama pull tinyllama
# OR for better answers (needs 16GB+ RAM):
# ollama pull llama2
```

---

### 📦 Installation

1. **Clone the repository:**

```bash
git clone <your-repo-link>
cd <your-repo-name>
```

2. **Create and activate a virtual environment:**

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

3. **Install dependencies:**

```bash
pip install -r requirements.txt
```

---

## ⚙️ Running the Application

1. **Ensure Ollama is running** in the background.

2. **Open `app.py` and configure your model:**

```python
Settings.llm = Ollama(model="tinyllama", request_timeout=360.0)
# OR
# Settings.llm = Ollama(model="llama2", request_timeout=360.0)
```

3. **Run the Flask server:**

```bash
python app.py
```

4. **Open your browser** and navigate to:

```
http://127.0.0.1:5000/
```

---

## 🧪 How to Use

1. **Upload a document** (`.txt` or `.pdf`).
2. **Ask questions** about its content.
3. **Receive answers** generated by your local LLM using retrieved context.

---

## 📁 Project Structure

```
.
├── app.py                   # Flask backend logic
├── index.html               # Frontend user interface
├── requirements.txt         # Python dependencies
├── uploaded_docs/           # Auto-created folder for uploaded files
└── README.md                # This documentation
```

---

## ⚠️ Troubleshooting & Tips

- 🧠 **First query slow?** Initial LLM load takes time. Later queries will be faster.
- 🧠 **Memory errors?** Use `tinyllama` if your system has <16GB RAM.
- 🔁 **Uploading a new doc?** The old index gets reset each time.
- 🚨 **"No LLM response" errors?** Make sure Ollama is running before starting Flask.

---

## 👐 Contributing

Feel free to fork, raise issues, or submit PRs. This project is a great starting point for building more advanced AI-native tools.

---

## 🖤 Credits

- **LlamaIndex** - https://www.llamaindex.ai/
- **Ollama** - https://ollama.com/
- **HuggingFace** - https://huggingface.co/

---

## 📜 License

MIT License — use it, tweak it, build on it.

---

> Built with 💻 + 🖤 for developers who want private, powerful AI tools without relying on the cloud.
